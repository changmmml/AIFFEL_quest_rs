{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f92dd75",
   "metadata": {},
   "source": [
    "# [Quest] MainQuest 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "152e4520",
   "metadata": {},
   "source": [
    "## 1. Transformer와 비교해 변경이 필요한 부분을 서술하였다.\n",
    "- 제출 노트북 파일 첫부분에 텍스트 블럭으로 서술합니다. 아키텍쳐 상 변경사항을 블럭단위로 서술합니다.\n",
    "- 코드블럭에 변경사항을 주석으로 표시합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bbbcb",
   "metadata": {},
   "source": [
    "### 비교 및 아키텍쳐 상 변경사항\n",
    "1. GPT-1은 encoder를 사용하지 않는다. (Decoder-only)\n",
    "2. GPT-1의 decoder는 'Masked Multi Self Attention' - LN - 'Feed Forward' - LN - 출력 으로 이루어져있다.\\\n",
    "즉, 인코더와 디코더의 입력을 처리하는 encoder-decoder 레이어가 없다.\n",
    "3. GPT-1은 주어진 단어들을 순차적으로 처리하는 AR 모델이므로, 이전 단어들만을 고려해서 다음 단어를 예측한다.\\\n",
    "= create_padding_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4009bf",
   "metadata": {},
   "source": [
    "**GPT-1 decoder 함수 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167e2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463cdb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수 (변경사항 X)\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db2118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 헤드 어텐션 함수 (변경사항 X)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7954782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 마스킹 (변경사항 X)\n",
    "def create_padding_mask(x):\n",
    "    \"\"\"\n",
    "    주어진 시퀀스에 대해 패딩 마스크를 생성하는 함수\n",
    "    - 패딩된 부분은 1로, 유효한 부분은 0으로 마스크\n",
    "    \"\"\"\n",
    "    padding_mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return padding_mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a10f056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 룩 어헤드 마스킹 (변경사항 X) - AR모델 기반일 때 causal mask로 사용\n",
    "def create_look_ahead_mask(x):\n",
    "    \"\"\"\n",
    "    룩 어헤드 마스크와 패딩 마스크를 결합하는 함수.\n",
    "    - 현재 단어 이후의 단어들을 마스크하고,\n",
    "    - 패딩된 부분을 무시하는 마스크를 결합합니다.\n",
    "    \n",
    "    :param x: 입력 시퀀스\n",
    "    :return: 결합된 마스크 (Causal + Padding Mask)\n",
    "    \"\"\"\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    batch_size = tf.shape(x)[0]  # 배치 크기 구하기\n",
    "    \n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    look_ahead_mask = look_ahead_mask[tf.newaxis, tf.newaxis, :, :]\n",
    "    \n",
    "    #look_ahead_mask를 배치 크기에 맞게 확장: (batch_size, 1, seq_len, seq_len)\n",
    "    look_ahead_mask = tf.tile(look_ahead_mask, [batch_size, 1, 1, 1])\n",
    "    \n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7851c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "        # inputs가 2D 텐서일 경우, 임베딩을 통해 3D 텐서로 변환\n",
    "        if len(tf.shape(inputs)) == 2:  # (batch_size, seq_len)\n",
    "            seq_len = tf.shape(inputs)[1]\n",
    "            d_model = 512  # d_model은 여러분의 모델에 맞게 설정\n",
    "            embedding_layer = tf.keras.layers.Embedding(input_dim=5000, output_dim=d_model)  # 예시로 5000 단어사전 사용\n",
    "            inputs = embedding_layer(inputs)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # inputs의 시퀀스 길이에 맞춰 pos_encoding을 자르고\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        pos_encoding = self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # pos_encoding을 inputs의 batch_size에 맞게 브로드캐스트\n",
    "        batch_size = tf.shape(inputs)[0]  # inputs의 배치 크기\n",
    "        pos_encoding = tf.broadcast_to(pos_encoding, [batch_size, seq_len, tf.shape(inputs)[2]])\n",
    "\n",
    "        # pos_encoding과 inputs을 더하기 전에 dtype을 맞추기 위해 cast 처리\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "\n",
    "        # inputs와 pos_encoding을 더하기\n",
    "        return inputs + pos_encoding  # 여기서 pos_encoding은 이미 inputs에 맞게 브로드캐스트됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf506a9",
   "metadata": {},
   "source": [
    "- GPT-1은 encoder를 사용하지 않는다. = Decoder-only (변경사항 O)\n",
    "- 파라미터 enc_outputs 삭제 (입력 시퀀스를 인코딩할 때 사용하는데, GPT-1는 decoder만 사용)\n",
    "- 파라미터 padding_mask 삭제 (주로 Encoder-Decoder 구조에서 필요)\\\n",
    "    GPT-1는 AR모델 기반이므로 Causal mask 사용해서 padding 처리 (2.에서 구현)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555e357",
   "metadata": {},
   "source": [
    "- GPT-1의 decoder는 'Masked Multi Self Attention' - LN - 'Feed Forward' - LN - 출력 으로 이루어져있다.\\\n",
    "즉, 인코더와 디코더의 입력을 처리하는 encoder-decoder attention 레이어가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c721029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어 함수 (변경사항 O)\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다. (Transformer에서 인코더-디코더 어텐션 삭제)\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\") # 현재 입력 (이전 단어들의 임베딩)\n",
    "  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\") # 마스크\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 드롭 아웃\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    \n",
    "  # 잔차 연결 + LayerNormalization\n",
    "  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 잔차 연결 + LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, look_ahead_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b62f10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 (변경사항 O)\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  \n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name='inputs') # 현재 단어의 인덱스 \n",
    "  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask') # 마스크\n",
    "\n",
    "  # Dropout\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(inputs)\n",
    "\n",
    "  # Decoder Layer 반복\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, look_ahead_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, look_ahead_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c3f11",
   "metadata": {},
   "source": [
    "## 2. 모델의 입력 형태에 맞게 전처리를 수행하였다.\n",
    "- Decoder 기반의 생성모델임을 감안하여 챗봇 데이터를 변형합니다.\n",
    "- 이번 과제는 pretrain을 위한 데이터셋과 학습만 고려합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4609e46",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 전처리 과정\n",
    "1. 디코더 모델의 입력 데이터로 적합한 형태로 변경\n",
    "2. 텍스트 시퀀스 토큰화\n",
    "3. Causal Mask (AR 마스크)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a01934",
   "metadata": {},
   "source": [
    "**데이터 로드 및 공백, 특수문자 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce02192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da0ac547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변을 전처리하는 함수\n",
    "def preprocess_sentence(text):\n",
    "    \"\"\"\n",
    "    입력된 텍스트에서 공백과 특수문자를 제거하는 함수\n",
    "    - 불필요한 공백 제거\n",
    "    - 특수문자 및 숫자 제거\n",
    "    \"\"\"\n",
    "    # 단어와 구두점(punctuation) 사이 공백\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    # 특수문자 및 숫자 제거\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)  # 한글, 공백만 남기고 제거\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 여러 공백을 하나로 치환 후 양쪽 공백 제거\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bd98869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일을 로드하고, 질문, 답변, 라벨을 전처리하는 함수\n",
    "def load_conversations(file_path):\n",
    "    inputs, outputs, labels = [], [], []\n",
    "    \n",
    "    # CSV 파일을 읽어서 각 줄을 처리\n",
    "    with open(file_path, mode='r', encoding='utf-8', errors='ignore') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # 헤더가 있을 경우 첫 줄을 건너뛰기\n",
    "        \n",
    "        for row in reader:\n",
    "            if len(row) == 3:  # Q, A, label이 존재하는지 확인\n",
    "                question, answer, label = row\n",
    "                # 질문과 답변 전처리\n",
    "                inputs.append(preprocess_sentence(question))\n",
    "                outputs.append(preprocess_sentence(answer))\n",
    "                labels.append(label)  # 라벨도 저장\n",
    "    \n",
    "    return inputs, outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba923ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../aiffel/transformer_chatbot/data/ChatbotData .csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf4f638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "questions, answers, lables = load_conversations(file_path)\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8989ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 10번째 질문 샘플: 시간낭비인데 자꾸 보게됨\n",
      "전처리 후의 10번째 답변 샘플: 시간을 정하고 해보세요\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 10번째 질문 샘플: {}'.format(questions[9]))\n",
    "print('전처리 후의 10번째 답변 샘플: {}'.format(answers[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580cebfe",
   "metadata": {},
   "source": [
    "### 텍스트 시퀀스 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b724e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8743908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8144]\n",
      "END_TOKEN의 번호 : [8145]\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e28ff37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8146\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size + 2  # 시작/종료 토큰을 고려하여 +2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8e3784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 10번째 질문 샘플: [772, 7594, 195, 79, 177, 362, 1251]\n",
      "정수 인코딩 후의 10번째 답변 샘플: [343, 3965, 13, 29]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 10번째 질문 샘플: {}'.format(tokenizer.encode(questions[9])))\n",
    "print('정수 인코딩 후의 10번째 답변 샘플: {}'.format(tokenizer.encode(answers[9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff4e6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변을 Subword 토큰화\n",
    "def tokenize_and_add_tokens(questions, answers):\n",
    "    tokenized_questions = []\n",
    "    tokenized_answers = []\n",
    "    \n",
    "    for question, answer in zip(questions, answers):\n",
    "        # [START]와 [END] 토큰 추가\n",
    "        tokenized_question = [START_TOKEN[0]] + tokenizer.encode(question) + [END_TOKEN[0]]\n",
    "        tokenized_answer = [START_TOKEN[0]] + tokenizer.encode(answer) + [END_TOKEN[0]]\n",
    "        \n",
    "        tokenized_questions.append(tokenized_question)\n",
    "        tokenized_answers.append(tokenized_answer)\n",
    "    \n",
    "    return tokenized_questions, tokenized_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a96b86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_add_tokens(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa1a9f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10번째 질문 샘플: [8144, 772, 7594, 195, 79, 177, 362, 1251, 8145]\n",
      "10번째 질문 샘플: [8144, 343, 3965, 13, 29, 8145]\n"
     ]
    }
   ],
   "source": [
    "# 앞에 [START], 뒤에 [END] 토큰 결합 확인\n",
    "print('10번째 질문 샘플: {}'.format(questions[9]))\n",
    "print('10번째 질문 샘플: {}'.format(answers[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b9260",
   "metadata": {},
   "source": [
    "## 3. 모델의 입력 블럭을 GPT 논문에 기반하여 수정하였다.\n",
    "- 모델의 input이 정상적으로 구성되었는지 확인합니다.\n",
    "- 데이터에 위치 정보를 추가하는 과정을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029da6e3",
   "metadata": {},
   "source": [
    "### 디코더 모델에 입력 데이터로 적합한 형식으로 변환\n",
    "- GPT-1 모델 입력 데이터 형식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8c77a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변을 하나의 시퀀스로 결합하여 모델에 적합한 형식으로 변환\n",
    "def prepare_gpt1_input(tokenized_questions, tokenized_answers):\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "    \n",
    "    for question, answer in zip(tokenized_questions, tokenized_answers):\n",
    "        # 입력: question + [END] + answer\n",
    "        input_sequence = question + answer[1:]  # answer[1:]는 [START] 토큰 제외\n",
    "        target_sequence = answer  # 모델은 answer의 시작 부분부터 예측\n",
    "        \n",
    "        input_sequences.append(input_sequence)\n",
    "        target_sequences.append(target_sequence)\n",
    "    \n",
    "    return input_sequences, target_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f9356d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "050094db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습에 사용할 입력 데이터 준비\n",
    "input_sequences, target_sequences = prepare_gpt1_input(questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6bad65",
   "metadata": {},
   "source": [
    "적절한 max_len 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc5bc735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences의 가장 긴 길이: 38\n",
      "Input sequences의 평균 길이: 12.925230482956948\n",
      "Target sequences의 가장 긴 길이: 29\n",
      "Target sequences의 평균 길이: 6.806140573458513\n"
     ]
    }
   ],
   "source": [
    "# Input sequences의 길이 계산\n",
    "input_lengths = [len(seq) for seq in input_sequences]\n",
    "max_input_len = max(input_lengths)\n",
    "avg_input_len = sum(input_lengths) / len(input_lengths)\n",
    "\n",
    "# Target sequences의 길이 계산\n",
    "target_lengths = [len(seq) for seq in target_sequences]\n",
    "max_target_len = max(target_lengths)\n",
    "avg_target_len = sum(target_lengths) / len(target_lengths)\n",
    "\n",
    "print(\"Input sequences의 가장 긴 길이:\", max_input_len)\n",
    "print(\"Input sequences의 평균 길이:\", avg_input_len)\n",
    "\n",
    "print(\"Target sequences의 가장 긴 길이:\", max_target_len)\n",
    "print(\"Target sequences의 평균 길이:\", avg_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da1707d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 각 시퀀스의 길이를 계산\n",
    "input_lengths = [len(seq) for seq in input_sequences]\n",
    "target_lengths = [len(seq) for seq in target_sequences]\n",
    "\n",
    "# 히스토그램 그리기\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Input sequences 히스토그램\n",
    "plt.subplot(1, 2, 1)  # (행, 열, 위치)\n",
    "plt.hist(input_lengths, bins=range(min(input_lengths), max(input_lengths) + 2), edgecolor='black', alpha=0.7)\n",
    "plt.title('Input Sequences Length Distribution')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Target sequences 히스토그램\n",
    "plt.subplot(1, 2, 2)  # (행, 열, 위치)\n",
    "plt.hist(target_lengths, bins=range(min(target_lengths), max(target_lengths) + 2), edgecolor='black', alpha=0.7)\n",
    "plt.title('Target Sequences Length Distribution')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 전체 히스토그램 표시\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476224d",
   "metadata": {},
   "source": [
    "input_sequences, target_sequences의 합일 때 적절한 max_len의 값을 40으로 그대로 잡아도 괜찮을 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f48cdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 40  # 시퀀스 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bd93c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩된 입력 시퀀스 (10번째): [8144  772 7594  195   79  177  362 1251 8145  343 3965   13   29 8145\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "패딩된 출력 시퀀스 (10번째): [8144  343 3965   13   29 8145    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 입력과 출력 시퀀스에 대한 패딩 추가\n",
    "inputs = pad_sequences(input_sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')\n",
    "targets = pad_sequences(target_sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# 패딩된 시퀀스 출력\n",
    "print(\"패딩된 입력 시퀀스 (10번째):\", inputs[9])\n",
    "print(\"패딩된 출력 시퀀스 (10번째):\", targets[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee4500",
   "metadata": {},
   "source": [
    "**변형 완료**\n",
    "\n",
    "입력 시퀀스 : [START] inputs [END] target [END] [PAD] ...\n",
    "\n",
    "출력 시퀀스 : [START] target [END] [PAD] ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60bb4a",
   "metadata": {},
   "source": [
    "### Causal Mask (AR 마스크)\n",
    "- 일전에 정의했던 `create_padding_mask()` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1bfd0633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 40)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6424c2a2",
   "metadata": {},
   "source": [
    "**교사 강요**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df44e29d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_381/3777036633.py:11 map_fn  *\n        look_ahead_mask = create_look_ahead_mask(inputs)\n    /tmp/ipykernel_381/381776603.py:11 create_look_ahead_mask  *\n        seq_len = tf.shape(x)[1]\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1041 _slice_helper\n        return strided_slice(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1214 strided_slice\n        op = gen_array_ops.strided_slice(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:10538 strided_slice\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3561 _create_op_internal\n        ret = Operation(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: slice index 1 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_381/3777036633.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 데이터셋에 map_fn을 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1860\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1861\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4979\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4980\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4981\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4982\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4983\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4218\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4219\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3148\u001b[0m          \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m     \"\"\"\n\u001b[0;32m-> 3150\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   3151\u001b[0m         *args, **kwargs)\n\u001b[1;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4193\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   4194\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4195\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4196\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4123\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4124\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4125\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4126\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4127\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /tmp/ipykernel_381/3777036633.py:11 map_fn  *\n        look_ahead_mask = create_look_ahead_mask(inputs)\n    /tmp/ipykernel_381/381776603.py:11 create_look_ahead_mask  *\n        seq_len = tf.shape(x)[1]\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1041 _slice_helper\n        return strided_slice(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1214 strided_slice\n        op = gen_array_ops.strided_slice(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:10538 strided_slice\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3561 _create_op_internal\n        ret = Operation(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: slice index 1 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n"
     ]
    }
   ],
   "source": [
    "# tf.data.Dataset 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "\n",
    "# 데이터셋을 배치 단위로 묶고, look_ahead_mask를 추가\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 데이터를 적절히 매핑\n",
    "def map_fn(inputs, target):\n",
    "    # Look-ahead 마스크 생성\n",
    "    look_ahead_mask = create_look_ahead_mask(inputs)\n",
    "    padding_mask = create_padding_mask(inputs)\n",
    "    \n",
    "    # (inputs, look_ahead_mask) 형태로 반환\n",
    "    return (inputs, look_ahead_mask), target  # 모델이 두 개의 입력을 받을 수 있도록\n",
    "\n",
    "# 데이터셋에 map_fn을 적용\n",
    "dataset = dataset.map(map_fn).cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da2384",
   "metadata": {},
   "source": [
    "### 포지셔널 인코딩 (위치 정보)\n",
    "- 일전에 정의했던 `PositionalEncoding()` 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4f8ff",
   "metadata": {},
   "source": [
    "## 4. GPT 모델을 정상적으로 구성하였다.(model.summary, model.fit 결과 캡쳐 첨부)\n",
    "- 노드의 transformer 코드를 수정하여 GPT1 모델을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b73ed1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_1(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"gpt_1\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  \n",
    "  # 룩 어헤드 마스크 입력 정의\n",
    "  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")  # look_ahead_mask는 inputs 기반으로 생성\n",
    "    \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))  # 차원 크기 (d_model)로 스케일링\n",
    "  \n",
    "  # 포지셔널 인코딩을 입력 임베딩에 추가\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)  # 포지셔널 인코딩 추가\n",
    "\n",
    "  # Dropout\n",
    "  embeddings = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 디코더\n",
    "  outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[embeddings, look_ahead_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, look_ahead_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18551ff",
   "metadata": {},
   "source": [
    "**모델 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a6c6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gpt_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 512)    4170752     inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, None, 512)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoding (Positional (None, None, 512)    0           tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 512)    0           positional_encoding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (InputLayer)    [(None, 1, None, Non 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    3155968     dropout[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8146)   4178898     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,505,618\n",
      "Trainable params: 11,505,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 디코더의 층의 개수\n",
    "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = gpt_1(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1640b",
   "metadata": {},
   "source": [
    "**손실 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dadcfc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d7b535",
   "metadata": {},
   "source": [
    "**커스텀 된 학습률**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f50a4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9a3bc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0bc0a",
   "metadata": {},
   "source": [
    "**모델 컴파일**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d9fd55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ecf8649b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:787 train_step\n        y_pred = self(x, training=True)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py:199 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer gpt_1 expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 40) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_381/206070047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3457\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3458\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3459\u001b[0;31m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[1;32m   3460\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3379\u001b[0m           expand_composites=True)\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3381\u001b[0;31m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[1;32m   3382\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[1;32m   3383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:787 train_step\n        y_pred = self(x, training=True)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py:199 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer gpt_1 expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 40) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa17c1",
   "metadata": {},
   "source": [
    "## 5. 입력에 따른 출력이 생성되었다.\n",
    "- 출력 결과물의 수준에 상관없이 모델이 정상적으로 동작하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed578e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation('오늘 뭐할까?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccd6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
